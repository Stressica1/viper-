#!/usr/bin/env python3
"""
# Rocket OPTIMIZED MARKET DATA STREAMER
High-performance market data fetching with advanced caching and optimization

This optimized version includes:
    pass
- Asynchronous batch data fetching
- Intelligent caching with TTL and LRU eviction
- Connection pooling and rate limiting
- Data compression and memory optimization
- Real-time streaming with WebSocket support
- Predictive data prefetching
- Multi-exchange support with failover
"""

import os
import asyncio
import logging
import aiohttp
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from collections import OrderedDict
import threading
import time
import ccxt
import ccxt.async_support as ccxt_async
from concurrent.futures import ThreadPoolExecutor
import gzip
import pickle
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass"""
class CacheEntry:
    """Cache entry with TTL and metadata"""
    data: Any
    timestamp: datetime
    ttl: int  # Time to live in seconds
    access_count: int = 0
    last_accessed: datetime = None
    compressed: bool = False
    data_hash: str = ""

    def __post_init__(self):
        if self.last_accessed is None:
            self.last_accessed = self.timestamp

    def is_expired(self) -> bool:
        return (datetime.now() - self.timestamp).seconds > self.ttl

    def is_stale(self) -> bool:
        return (datetime.now() - self.last_accessed).seconds > self.ttl // 2

@dataclass
class DataRequest:
    """Market data request with priority and metadata"""
    symbol: str
    timeframe: str
    limit: int
    priority: int = 1  # 1=low, 5=high
    callback: Optional[callable] = None
    metadata: Dict[str, Any] = None"""

class OptimizedMarketDataStreamer:
    """Optimized market data streamer with advanced caching and performance features""""""

    def __init__(self, cache_size_mb: int = 500, max_connections: int = 20):
        self.cache_size_mb = cache_size_mb
        self.max_connections = max_connections

        # Multi-exchange support
        self.exchanges = {}
        self.exchange_configs = {}

        # Advanced caching system
        self.cache = OrderedDict()  # LRU cache
        self.cache_lock = asyncio.Lock()
        self.current_cache_size = 0

        # Request queue and processing
        self.request_queue = asyncio.Queue()
        self.processing_tasks = []
        self.is_running = False

        # Performance metrics
        self.metrics = {
            'requests_processed': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'avg_response_time': 0.0,
            'errors': 0,
            'data_points_fetched': 0
        }

        # Connection pooling
        self.session_pool = {}
        self.session_lock = asyncio.Lock()

        # Predictive prefetching
        self.prefetch_patterns = {}
        self.symbol_correlations = {}

        logger.info(f"# Chart Optimized Market Data Streamer initialized (Cache: {cache_size_mb}MB, Connections: {max_connections})")

    async def initialize_exchanges(self) -> bool:
        """Initialize multiple exchanges with failover support""""""
        try:
            exchange_configs = [
                {
                    'name': 'bitget',
                    'class': ccxt_async.bitget,
                    'config': {
                        'apiKey': os.getenv('BITGET_API_KEY'),
                        'secret': os.getenv('BITGET_API_SECRET'),
                        'password': os.getenv('BITGET_API_PASSWORD'),
                        'enableRateLimit': True,
                        'options': {'defaultType': 'swap'},
                        'sandbox': False
                    }
                },
                # Add more exchanges here for failover
            ]

            for config in exchange_configs:
                try:
                    exchange = config['class'](config['config'])
                    await exchange.load_markets()

                    self.exchanges[config['name']] = exchange
                    self.exchange_configs[config['name']] = config

                    logger.info(f"# Check Exchange {config['name']} initialized")

                except Exception as e:
                    logger.warning(f"# Warning Failed to initialize {config['name']}: {e}")

            if not self.exchanges:
                logger.error("# X No exchanges successfully initialized")
                return False

            # Set primary exchange
            self.primary_exchange = list(self.exchanges.keys())[0]
            logger.info(f"# Target Primary exchange set to: {self.primary_exchange}")

            return True

        except Exception as e:
            logger.error(f"# X Error initializing exchanges: {e}")
            return False

    async def start_streaming(self):
        """Start the optimized data streaming system""""""
        try:
            self.is_running = True

            # Start processing tasks
            for i in range(self.max_connections):
                task = asyncio.create_task(self._process_requests())
                self.processing_tasks.append(task)

            # Start maintenance tasks
            asyncio.create_task(self._cache_maintenance())
            asyncio.create_task(self._metrics_updater())

            logger.info(f"# Rocket Streaming started with {len(self.processing_tasks)} processing tasks")

        except Exception as e:
            logger.error(f"# X Error starting streaming: {e}")
            self.is_running = False

    async def stop_streaming(self):
        """Stop the streaming system gracefully""""""
        try:
            self.is_running = False

            # Cancel processing tasks
            for task in self.processing_tasks:
                task.cancel()

            # Close exchange connections
            for name, exchange in self.exchanges.items():
                try:
                    await exchange.close()
                except Exception:
                    pass

            # Close HTTP sessions
            async with self.session_lock:
                for session in self.session_pool.values():
                    await session.close()

            logger.info("ðŸ›‘ Streaming stopped gracefully")

        except Exception as e:
            logger.error(f"# X Error stopping streaming: {e}")

    async def fetch_market_data(self, symbol: str, timeframe: str = '1h',):
(                              limit: int = 100, use_cache: bool = True) -> Optional[pd.DataFrame]
        """Fetch market data with intelligent caching and optimization""""""
        try:
            start_time = time.time()

            # Check cache first
            if use_cache:
                cached_data = await self._get_cached_data(symbol, timeframe, limit)
                if cached_data is not None:
                    self.metrics['cache_hits'] += 1
                    response_time = time.time() - start_time
                    self._update_response_time(response_time)
                    return cached_data

            self.metrics['cache_misses'] += 1

            # Create request
            request = DataRequest()
                symbol=symbol,
                timeframe=timeframe,
                limit=limit,
                priority=3 if timeframe in ['1m', '5m'] else 2
(            )

            # Add to queue
            await self.request_queue.put(request)

            # Wait for result (with timeout)
            try:
                result = await asyncio.wait_for()
                    self._wait_for_request_result(request),
                    timeout=30.0
(                )

                if result:
                    # Cache the result
                    await self._cache_data(symbol, timeframe, limit, result)

                    response_time = time.time() - start_time
                    self._update_response_time(response_time)
                    self.metrics['requests_processed'] += 1

                    return result

            except asyncio.TimeoutError
                logger.warning(f"â° Timeout fetching {symbol} {timeframe}")
                self.metrics['errors'] += 1

            return None

        except Exception as e:
            logger.error(f"# X Error fetching market data: {e}")
            self.metrics['errors'] += 1
            return None

    async def _process_requests(self):
        """Process data requests from the queue"""
        while self.is_running:"""
            try:
                # Get request from queue
                request = await self.request_queue.get()

                # Process request
                result = await self._execute_data_request(request)

                # Store result for waiting caller
                if hasattr(request, 'result_future'):
                    request.result_future.set_result(result)

                self.request_queue.task_done()

            except Exception as e:
                logger.error(f"# X Error processing request: {e}")
                continue

    async def _execute_data_request(self, request: DataRequest) -> Optional[pd.DataFrame]
        """Execute a single data request with failover"""""":
        try:
            # Try primary exchange first
            result = await self._fetch_from_exchange()
                self.exchanges[self.primary_exchange],
                request.symbol,
                request.timeframe,
                request.limit
(            )

            if result is not None:
                return result

            # Try failover exchanges
            for name, exchange in self.exchanges.items():
                if name != self.primary_exchange:
                    try:
                        result = await self._fetch_from_exchange()
                            exchange, request.symbol, request.timeframe, request.limit
(                        )
                        if result is not None:
                            logger.info(f"ðŸ”„ Failover successful with {name}")
                            return result
                    except Exception as e:
                        logger.warning(f"# Warning Failover to {name} failed: {e}")
                        continue

            logger.warning(f"# X All exchanges failed for {request.symbol}")
            return None

        except Exception as e:
            logger.error(f"# X Error executing data request: {e}")
            return None

    async def _fetch_from_exchange(self, exchange: ccxt_async.Exchange,):
(                                 symbol: str, timeframe: str, limit: int) -> Optional[pd.DataFrame]
        """Fetch data from a specific exchange with optimization""""""
        try:
            # Convert timeframe to exchange format if needed
            exchange_timeframe = self._convert_timeframe(timeframe)

            # Fetch OHLCV data
            ohlcv = await exchange.fetch_ohlcv(symbol, exchange_timeframe, limit=limit)

            if not ohlcv or len(ohlcv) == 0:
                return None

            # Convert to DataFrame efficiently
            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])

            # Optimize data types
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            for col in ['open', 'high', 'low', 'close', 'volume']:
                df[col] = df[col].astype(np.float32)

            df.set_index('timestamp', inplace=True)

            # Add basic indicators for immediate use
            df = self._add_basic_indicators(df)

            self.metrics['data_points_fetched'] += len(df)

            return df

        except Exception as e:
            logger.error(f"# X Error fetching from exchange: {e}")
            return None

    def _add_basic_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add basic indicators for immediate use""""""
        try:
            # Simple moving averages
            df['sma_20'] = df['close'].rolling(window=20).mean()
            df['sma_50'] = df['close'].rolling(window=50).mean()

            # RSI
import ta

            df['rsi'] = ta.momentum.rsi(df['close'], window=14)

            # Returns
            df['returns'] = df['close'].pct_change()

            return df

        except Exception as e:
            logger.warning(f"# Warning Error adding basic indicators: {e}")
            return df

    async def _get_cached_data(self, symbol: str, timeframe: str, limit: int) -> Optional[pd.DataFrame]
        """Get data from cache with TTL checking"""""":
        try:
            cache_key = self._generate_cache_key(symbol, timeframe, limit)

            async with self.cache_lock:
                if cache_key in self.cache:
                    entry = self.cache[cache_key]

                    if not entry.is_expired():
                        # Update access statistics
                        entry.access_count += 1
                        entry.last_accessed = datetime.now()

                        # Move to end (most recently used)
                        self.cache.move_to_end(cache_key)

                        # Decompress if needed
                        if entry.compressed:
                            return self._decompress_data(entry.data)
                        else:
                            return entry.data

                    else:
                        # Remove expired entry
                        del self.cache[cache_key]

            return None

        except Exception as e:
            logger.warning(f"# Warning Error getting cached data: {e}")
            return None

    async def _cache_data(self, symbol: str, timeframe: str, limit: int, data: pd.DataFrame):
        """Cache data with compression and size management""""""
        try:
            cache_key = self._generate_cache_key(symbol, timeframe, limit)

            # Calculate data size
            data_size = data.memory_usage(deep=True).sum()

            # Compress if beneficial
            compressed_data = None
            if data_size > 100000:  # 100KB
                compressed_data = self._compress_data(data)
                if compressed_data and len(compressed_data) < data_size * 0.7:  # 30% compression
                    use_compressed = True
                else:
                    use_compressed = False
            else:
                use_compressed = False

            # Create cache entry
            entry = CacheEntry()
                data=compressed_data if use_compressed else data,
                timestamp=datetime.now(),
                ttl=self._get_ttl_for_timeframe(timeframe),
                compressed=use_compressed,
                data_hash=self._calculate_data_hash(data)
(            )

            async with self.cache_lock:
                # Check cache size limit
                while self.current_cache_size + data_size > self.cache_size_mb * 1024 * 1024:
                    if not self.cache:
                        break
                    # Remove least recently used
                    removed_key, removed_entry = self.cache.popitem(last=False)
                    if removed_entry.compressed:
                        removed_size = len(removed_entry.data)
                    else:
                        removed_size = removed_entry.data.memory_usage(deep=True).sum()
                    self.current_cache_size -= removed_size

                # Add new entry
                self.cache[cache_key] = entry
                self.current_cache_size += data_size

        except Exception as e:
            logger.warning(f"# Warning Error caching data: {e}")

    def _generate_cache_key(self, symbol: str, timeframe: str, limit: int) -> str:
        """Generate unique cache key"""
        return f"{symbol}_{timeframe}_{limit}"

    def _get_ttl_for_timeframe(self, timeframe: str) -> int:
        """Get appropriate TTL for timeframe"""
        ttl_map = {
            '1m': 60,      # 1 minute
            '5m': 300,     # 5 minutes
            '15m': 900,    # 15 minutes
            '1h': 3600,    # 1 hour
            '4h': 14400,   # 4 hours
            '1d': 86400,   # 1 day
        }
        return ttl_map.get(timeframe, 3600)  # Default 1 hour"""

    def _convert_timeframe(self, timeframe: str) -> str:
        """Convert timeframe to exchange format if needed"""
        # Most exchanges use the same format
        return timeframe"""

    def _compress_data(self, data: pd.DataFrame) -> Optional[bytes]
        """Compress DataFrame for storage""":"""
        try:
            return gzip.compress(pickle.dumps(data))
        except Exception as e:
            logger.warning(f"# Warning Error compressing data: {e}")
            return None

    def _decompress_data(self, compressed_data: bytes) -> Optional[pd.DataFrame]
        """Decompress DataFrame""":"""
        try:
            return pickle.loads(gzip.decompress(compressed_data))
        except Exception as e:
            logger.warning(f"# Warning Error decompressing data: {e}")
            return None

    def _calculate_data_hash(self, data: pd.DataFrame) -> str:
        """Calculate hash of data for change detection""""""
        try:
            data_str = str(data.values.tobytes())
            return hashlib.md5(data_str.encode()).hexdigest()
        except Exception:
            return ""

    def _update_response_time(self, response_time: float):
        """Update average response time metric""""""
        try:
            current_avg = self.metrics['avg_response_time']
            total_requests = self.metrics['requests_processed']

            if total_requests > 0:
                self.metrics['avg_response_time'] = (current_avg * (total_requests - 1) + response_time) / total_requests
            else:
                self.metrics['avg_response_time'] = response_time

        except Exception as e:
            logger.warning(f"# Warning Error updating response time: {e}")

    async def _cache_maintenance(self):
        """Periodic cache maintenance"""
        while self.is_running:"""
            try:
                await asyncio.sleep(300)  # 5 minutes

                async with self.cache_lock:
                    expired_keys = []

                    for key, entry in self.cache.items():
                        if entry.is_expired():
                            expired_keys.append(key)
                        elif entry.is_stale():
                            # Update access patterns for prefetching
                            await self._update_prefetch_patterns(key, entry)

                    # Remove expired entries
                    for key in expired_keys:
                        del self.cache[key]

                    if expired_keys:
                        logger.info(f"ðŸ§¹ Cleaned {len(expired_keys)} expired cache entries")

            except Exception as e:
                logger.warning(f"# Warning Error in cache maintenance: {e}")

    async def _update_prefetch_patterns(self, key: str, entry: CacheEntry):
        """Update prefetch patterns based on access patterns""""""
        try:
            symbol, timeframe, limit = key.split('_')

            if symbol not in self.prefetch_patterns:
                self.prefetch_patterns[symbol] = {}

            if timeframe not in self.prefetch_patterns[symbol]:
                self.prefetch_patterns[symbol][timeframe] = {
                    'access_count': 0,
                    'last_access': None,
                    'frequency': 0
                }

            pattern = self.prefetch_patterns[symbol][timeframe]
            pattern['access_count'] += entry.access_count

            if pattern['last_access']:
                time_diff = (entry.last_accessed - pattern['last_access']).seconds
                if time_diff > 0:
                    pattern['frequency'] = pattern['access_count'] / (time_diff / 3600)  # accesses per hour

            pattern['last_access'] = entry.last_accessed

        except Exception as e:
            logger.warning(f"# Warning Error updating prefetch patterns: {e}")

    async def _wait_for_request_result(self, request: DataRequest) -> Optional[pd.DataFrame]
        """Wait for request result (placeholder for actual implementation)"""
        # This would be implemented with actual result passing mechanism
        await asyncio.sleep(0.1)
        return None

    async def get_streaming_metrics(self) -> Dict[str, Any]
        """Get comprehensive streaming metrics"""""":
        try:
            cache_info = await self._get_cache_info()

            metrics = {
                'timestamp': datetime.now().isoformat(),
                'performance': self.metrics.copy(),
                'cache': cache_info,
                'exchanges': {
                    name: {
                        'status': 'active' if exchange in self.exchanges else 'inactive',
                        'markets_loaded': len(exchange.markets) if hasattr(exchange, 'markets') else 0
                    }
                    for name, exchange in self.exchanges.items():
                },
                'queue': {
                    'size': self.request_queue.qsize(),
                    'processing_tasks': len(self.processing_tasks)
                }
            }

            return metrics

        except Exception as e:
            logger.error(f"# X Error getting metrics: {e}")
            return {'error': str(e)}

    async def _get_cache_info(self) -> Dict[str, Any]
        """Get cache statistics"""""":
        try:
            async with self.cache_lock:
                total_entries = len(self.cache)
                total_size_mb = self.current_cache_size / (1024 * 1024)

                # Calculate hit rate
                total_requests = self.metrics['cache_hits'] + self.metrics['cache_misses']
                hit_rate = self.metrics['cache_hits'] / max(total_requests, 1)

                return {
                    'total_entries': total_entries,
                    'total_size_mb': total_size_mb,
                    'hit_rate': hit_rate,
                    'utilization_percent': (total_size_mb / self.cache_size_mb) * 100
                }

        except Exception as e:
            logger.warning(f"# Warning Error getting cache info: {e}")
            return {'error': str(e)}

async def test_optimized_streamer():
    """Test the optimized market data streamer"""

    streamer = OptimizedMarketDataStreamer(cache_size_mb=100, max_connections=5)"""

    if not await streamer.initialize_exchanges():
        return

    await streamer.start_streaming()

    # Test data fetching
    symbols = ['BTCUSDT', 'ETHUSDT']
    timeframes = ['1h', '4h']


    for symbol in symbols:
        for timeframe in timeframes:
            pass

            # First fetch (cache miss)
            start_time = time.time()
            data = await streamer.fetch_market_data(symbol, timeframe, limit=50)
            first_fetch_time = time.time() - start_time

            # Second fetch (cache hit)
            start_time = time.time()
            data2 = await streamer.fetch_market_data(symbol, timeframe, limit=50)
            second_fetch_time = time.time() - start_time

            if data is not None:
                print(f"   # Check First fetch: {first_fetch_time:.2f}s ({len(data)} bars)")
                print(f"   # Check Second fetch: {second_fetch_time:.2f}s (cached)")
                print(f"   ðŸ“ˆ Price range: {data['close'].min():.2f} - {data['close'].max():.2f}")
            else:
                pass

    # Get metrics
    metrics = await streamer.get_streaming_metrics()

    await streamer.stop_streaming()

if __name__ == "__main__":
    asyncio.run(test_optimized_streamer())
